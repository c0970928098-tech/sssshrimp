import os
import json
import random
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoProcessor, AutoModelForCausalLM, AutoModel ,LogitsProcessor
import torchaudio
import pandas as pd
from peft import LoraConfig, get_peft_model, TaskType
from nltk.translate.meteor_score import meteor_score

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

SAVE_ROOT = "/mnt/disk5/1021_new_test/"
PROJECTOR_SAVE_DIR = os.path.join(SAVE_ROOT, "projector")
LORA_SAVE_DIR = os.path.join(SAVE_ROOT, "lora_adapters")
CAPTION_SAVE_DIR = os.path.join(SAVE_ROOT, "captions")

class Config:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    hidden_size = 3072
    max_length = 512

config = Config()

SYSTEM_PROMPT = (
                "You are Shrimpâ€‘LLM, an expert AI for analyzing shrimp tank underwater audio. You receive an audio segment together with a user question. Tasks:\n1) QA description â€” provide clear, factual, concise analysis of audible events (e.g., shrimp walking, water and background sound)\n"
            )

class AllowListLogitsProcessor(LogitsProcessor):
    """åªå…è¨±æ¨¡å‹åœ¨ allowed_token_ids è£¡é¸æ“‡ä¸‹ä¸€å€‹ tokenã€‚"""
    def __init__(self, allowed_token_ids):
        super().__init__()
        if isinstance(allowed_token_ids, torch.Tensor):
            self.allowed = allowed_token_ids.long()
        else:
            self.allowed = torch.tensor(allowed_token_ids).long()
    def __call__(self, input_ids, scores):
        mask = torch.full_like(scores, float('-inf'))
        mask[:, self.allowed] = 0
        return scores + mask

class AudioProjector(nn.Module):
    def __init__(self, in_dim=512, out_dim=3072, dropout=0.1, gate_init=-2.0, l2norm=True):
        super().__init__()
        self.ln = nn.LayerNorm(in_dim)
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, out_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(out_dim, out_dim),
        )
        self.gate = nn.Parameter(torch.tensor(gate_init))  # Î± = sigmoid(gate)
        self.l2norm = l2norm

    def forward(self, clap_feat):  # [B, 512]
        x = self.ln(clap_feat)
        y = self.mlp(x)  # [B, out_dim]
        if self.l2norm:
            y = y / (y.norm(dim=-1, keepdim=True) + 1e-6)
        alpha = torch.sigmoid(self.gate)  # scalar âˆˆ (0,1)
        return y, alpha

class MultiModalInstructionTuningModel(nn.Module):
    def __init__(self, clap_model_name, llm_model_name, config, tokenizer, clap_ckpt_path=None):
        super().__init__()
        self.config = config
        self.tokenizer = tokenizer

        # å…ˆç”¨ base æ¬Šé‡å»ºç«‹çµæ§‹
        self.clap_model = AutoModel.from_pretrained(clap_model_name).to(config.device)

        # âœ… è‹¥æä¾› finetune ckptï¼Œå°±è¼‰å…¥
        if clap_ckpt_path is not None:
            load_clap_weights_into_model(self.clap_model, clap_ckpt_path)

        # å†å‡çµ
        for p in self.clap_model.parameters():
            p.requires_grad = False

        self.audio_projector = AudioProjector(in_dim=512, out_dim=config.hidden_size, dropout=0.1, gate_init=-2.0, l2norm=True)
        self.llm = AutoModelForCausalLM.from_pretrained(
            llm_model_name,
            trust_remote_code=True,
            attn_implementation="eager"
        ).to(config.device)
        lora_config = LoraConfig(
            r=8, lora_alpha=16, lora_dropout=0.05,
            bias="none",
            target_modules=["self_attn.o_proj", "self_attn.qkv_proj"],
            task_type=TaskType.CAUSAL_LM
        )
        self.llm = get_peft_model(self.llm, lora_config)
        self.llm.resize_token_embeddings(len(tokenizer))

    def extract_audio_features(self, waveform, sample_rate=48000):
        if waveform.dim() == 2 and waveform.size(0) > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        elif waveform.dim() == 1:
            waveform = waveform.unsqueeze(0)

        if waveform.size(1) < sample_rate:
            repeat = sample_rate // waveform.size(1) + 1
            waveform = torch.cat([waveform] * repeat, dim=1)

        if sample_rate != 48000:
            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=48000)(waveform)

        inputs = processor(audios=waveform.cpu().numpy(), sampling_rate=48000, return_tensors="pt").to(self.config.device)
        with torch.no_grad():
            features = self.clap_model.get_audio_features(**inputs).squeeze(0)
        return features

    def forward(self, audio_features, prompts, captions):
        device = audio_features.device  # âœ… ç”¨ audio_features çš„ device

        # 1) æº–å‚™ teacher-forcing çš„è¼¸å…¥ï¼ˆprompt+captionï¼‰ï¼Œlabels åªè¨ˆ caption
        full_texts = [p + c for p, c in zip(prompts, captions)]
        tokenized = self.tokenizer(
            full_texts, return_tensors="pt",
            padding=True, truncation=True, max_length=self.config.max_length
        )
        input_ids = tokenized["input_ids"].to(device)
        attention_mask = tokenized["attention_mask"].to(device)

        # å»ºè­°ç”¨ã€ŒåŒä¸€æ¬¡ tokenize çš„çµæœã€ä¼° prompt é•·åº¦ï¼Œé¿å…æ¨¡æ¿ä¸ä¸€è‡´
        # é€™è£¡ç¶­æŒä½ çš„ä½œæ³•ä¹Ÿå¯ï¼›è‹¥è¦æ›´åš´è¬¹ï¼Œå¯å¦å¤–å° prompts åšä¸€æ¬¡ tokenize å†å–é•·åº¦
        prompt_lens = [len(self.tokenizer(p, truncation=True, max_length=self.config.max_length)["input_ids"]) for p in prompts]

        # 2) Projectorï¼ˆLN+MLP+Gateï¼‰
        projected_audio, alpha = self.audio_projector(audio_features)  # [B, H], scalar

        # 3) å–å¾—æ–‡å­—åµŒå…¥ä¸¦åœ¨ <end_of_audio> ä½ç½®åšã€Œæ®˜å·®æ³¨å…¥ã€
        inputs_embeds = self.llm.get_input_embeddings()(input_ids)   # [B, T, H]
        end_token_id = self.tokenizer.convert_tokens_to_ids("<end_of_audio>")

        B = input_ids.size(0)
        for b in range(B):
            pos = (input_ids[b] == end_token_id).nonzero(as_tuple=False)
            if len(pos) > 0:
                j = pos[0].item()
                inputs_embeds[b, j, :] = inputs_embeds[b, j, :] + alpha * projected_audio[b]

        # 4) å»º labelsï¼šé®æ‰ prompt å€æ®µ
        labels = input_ids.clone()
        for b, plen in enumerate(prompt_lens):
            labels[b, :plen] = -100

        # 5) å‰å‘è¨ˆç®—
        return self.llm(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels)
    
def score_labels_by_logprob(model, tokenizer, inputs_embeds, attention_mask, candidate_texts, device):
    """
    å°æ¯å€‹å€™é¸æ¨™ç±¤ï¼ˆå¯èƒ½å¤š tokenï¼‰è¨ˆç®—çºŒå¯«çš„ç¸½ logprobï¼Œå›å‚³ (best_label, scores_dict)
    é‡è¦ï¼šä¿ç•™ä½ å·²ç¶“åšå¥½çš„æ®˜å·®æ³¨å…¥çš„ inputs_embeds ä½œç‚ºå‰ç¶´ã€‚
    """
    E = inputs_embeds  # [1, T, H]
    A = attention_mask # [1, T]
    scores = {}

    for lbl in candidate_texts:
        ids = tokenizer(lbl, add_special_tokens=False)["input_ids"]
        if len(ids) == 0:
            scores[lbl] = float("-inf"); continue

        # é€ token ç´¯ç© logprob
        total_logp = 0.0
        cur_embeds = E
        cur_attn   = A

        for t in ids:
            # å–å¾—ä¸‹ä¸€æ­¥ logits
            with torch.no_grad():
                out = model.llm(inputs_embeds=cur_embeds, attention_mask=cur_attn, use_cache=False)
                # åªå–æœ€å¾Œä¸€å€‹ä½ç½®çš„ logits
                next_logits = out.logits[:, -1, :]  # [1, V]
                logp = torch.log_softmax(next_logits, dim=-1)[0, t].item()
                total_logp += logp

            # å°‡é¸å®š token çš„åµŒå…¥æ¥åˆ°åºåˆ—æœ«ç«¯ï¼Œæº–å‚™è©•åˆ†ä¸‹ä¸€å€‹ token
            tok_embed = model.llm.get_input_embeddings()(torch.tensor([[t]], device=device))  # [1,1,H]
            cur_embeds = torch.cat([cur_embeds, tok_embed], dim=1)
            cur_attn   = torch.cat([cur_attn, torch.ones_like(cur_attn[:, :1])], dim=1)

        scores[lbl] = total_logp

    # å–æœ€é«˜åˆ†æ¨™ç±¤
    best = max(scores.items(), key=lambda x: x[1])[0]
    return best, scores


class AudioInstructionDataset(Dataset):
    """
    æ”¯æ´ JSONL æ¬„ä½:
      - audio_path (or file_name)
      - question (user prompt)
      - answer_text (for QA)
      - label (for classification)
      - system_prompt (å¯é¸, è¦†è“‹å…¨åŸŸ SYSTEM_PROMPT)
    """
    def __init__(self, path, source_to_audio_dir, model, max_length=512):
        self.samples = []
        self.source_to_audio_dir = source_to_audio_dir or {}

        # è¼‰å…¥ JSONL
        with open(path, "r") as f:
            rows = [json.loads(line) for line in f if line.strip()]

        def _normalize_label(s: str):
            if not s:
                return s
            s = s.strip().lower()
            mapping = {
                "water and background sound": "water",
                "shrimp walking": "walk",
                "walking": "walk",
                "walk": "walk",
            }
            return mapping.get(s, s)
        
        def _append_wav_if_needed(p: str) -> str:
            base, ext = os.path.splitext(p)
            if ext == "":
                return base + ".wav"
            return p
    
        for row in rows:
            raw_audio = row.get("audio_path", "").strip()
            raw_audio = _append_wav_if_needed(raw_audio)

            question = (row.get("question") or "").strip()
            task = (row.get("task") or "qa").strip().lower()
            answer_text = row.get("answer_text")
            label = row.get("label")
            label_set = row.get("label_set", None)   # â† è®€å…¥ label_set
            source = row.get("meta", {}).get("source", "shrimp")

            # ---- æ±ºå®šç­”æ¡ˆ/æ¨™ç±¤ ----
            if task == "cls":
                # åˆ†é¡ä»»å‹™ï¼šåªæ¥å—å–®ä¸€æ¨™ç±¤
                label = _normalize_label(label)
                if not label:
                    print(f"[Skip] cls sample missing label: {raw_audio}")
                    continue
                caption = label  # è¨“ç·´ç›®æ¨™å°±æ˜¯å–®ä¸€æ¨™ç±¤
                # åªæœ‰ cls é¡Œä¿ç•™ label_setï¼ˆä¸¦æ­£è¦åŒ–ï¼‰ï¼›qa é¡Œä¸€å¾‹è¨­ None
                if label_set:
                    label_set = [_normalize_label(x) for x in label_set]
                else:
                    label_set = None
            elif task == "qa":
                # QA ä»»å‹™ï¼šåªæ¥å—è‡ªç„¶èªå¥ç­”æ¡ˆï¼Œä¸ä½¿ç”¨ label_set
                if not (answer_text and answer_text.strip()):
                    print(f"[Skip] qa sample missing answer_text: {raw_audio}")
                    continue
                caption = answer_text.strip()
                label_set = None  # ğŸ”’ é¿å… QA è¢«å—é™è§£ç¢¼å½±éŸ¿
            else:
                print(f"[Skip] unknown task '{task}': {raw_audio}")
                continue

            # ---- æ±ºå®šéŸ³æª”è·¯å¾‘ï¼ˆä¿æŒåŸé‚è¼¯ï¼‰----
            if os.path.isabs(raw_audio):
                audio_path = raw_audio
            else:
                candidate = os.path.join(os.path.dirname(path), raw_audio)
                if os.path.exists(candidate):
                    audio_path = candidate
                else:
                    audio_dir = self.source_to_audio_dir.get(source, "")
                    audio_path = os.path.join(audio_dir, raw_audio)

            if not os.path.exists(audio_path):
                base, _ = os.path.splitext(audio_path)
                wav_try = base + ".wav"
                if os.path.exists(wav_try):
                    audio_path = wav_try
                else:
                    print(f"[Skip] Missing audio: {audio_path}")
                    continue

            # ---- å»ºç«‹å°è©±æ¨£æœ¬ ----
            system_prompt = SYSTEM_PROMPT
            user_prompt = "<end_of_audio>\n" + question

            self.samples.append({
                "audio_path": audio_path,
                "source": source,
                "task": task,            # â˜… å­˜ task
                "label_set": label_set,  # â˜… åªæœ‰ cls æ‰æœƒæœ‰ï¼›qa æ˜¯ None
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                    {"role": "assistant", "content": caption + "<|end|>\n"}
                ]
            })
        print(f"âœ… Total valid samples: {len(self.samples)}  (from {path})")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        task = sample["task"]
        label_set = sample.get("label_set", None)  # â˜… æ–°å¢
        audio_path = sample["audio_path"]
        source = sample["source"]
        messages = sample["messages"]

        # prompt åˆ° <|assistant|> å‰
        prompt = model.tokenizer.apply_chat_template(
            messages[:2],
            add_generation_prompt=True,
            tokenize=False
        )
        caption = messages[2]["content"]

        try:
            waveform, sample_rate = torchaudio.load(audio_path)
            features = model.extract_audio_features(waveform, sample_rate)
            return features, prompt, caption, source, label_set, task
        except Exception as e:
            print(f"[LoadError] {audio_path}: {e}")
            return None, None, None, None


def load_clap_weights_into_model(clap_model: torch.nn.Module, ckpt_path: str):
    """
    å°‡æœ¬åœ° finetuned CLAP æ¬Šé‡è¼‰å…¥åˆ° AutoModel å»ºç«‹çš„ clap_modelã€‚
    å…¼å®¹å¹¾ç¨®å¸¸è¦‹æ ¼å¼ï¼š
    - ç´” state_dict
    - PyTorch Lightning: {"state_dict": ...}
    - DataParallel: key ä»¥ "module." é–‹é ­
    æœƒè‡ªå‹•åš key æ­£è¦åŒ–ä¸¦åªè¼‰å…¥å¯å°ä¸Šçš„æ¬Šé‡ï¼ˆstrict=Falseï¼‰ã€‚
    """
    if not os.path.exists(ckpt_path):
        print(f"[CLAP-Load] âŒ ckpt not found: {ckpt_path}")
        return

    print(f"[CLAP-Load] Loading CLAP checkpoint from: {ckpt_path}")
    ckpt = torch.load(ckpt_path, map_location="cpu")

    # å–å‡º state_dict
    if isinstance(ckpt, dict) and "state_dict" in ckpt:
        sd = ckpt["state_dict"]
    elif isinstance(ckpt, dict):
        sd = ckpt
    else:
        print("[CLAP-Load] âŒ Unsupported checkpoint format.")
        return

    # å¯èƒ½çš„å‰ç¶´éœ€è¦å»æ‰ï¼šmodule., model., clap_model., clap., audio_branch. ç­‰
    def normalize_key(k: str) -> str:
        for prefix in ["module.", "model.", "clap_model.", "clap.", "audio_backbone.", "audio_branch."]:
            if k.startswith(prefix):
                return k[len(prefix):]
        return k

    sd = {normalize_key(k): v for k, v in sd.items()}

    # åªä¿ç•™ clap_model ä¸­å­˜åœ¨çš„ key
    model_sd = clap_model.state_dict()
    filtered = {}
    matched, skipped = 0, 0
    for k, v in sd.items():
        if k in model_sd and model_sd[k].shape == v.shape:
            filtered[k] = v
            matched += 1
        else:
            skipped += 1

    missing, unexpected = clap_model.load_state_dict(filtered, strict=False)
    print(f"[CLAP-Load] matched: {matched}, skipped(by shape/key): {skipped}")
    if missing:
        print(f"[CLAP-Load] missing keys: {list(missing)[:10]} ... (total {len(missing)})")
    if unexpected:
        print(f"[CLAP-Load] unexpected keys: {list(unexpected)[:10]} ... (total {len(unexpected)})")
    print("[CLAP-Load] âœ… done.")

def collate_fn(batch):
    batch = [s for s in batch if s[0] is not None]
    if len(batch) == 0:
        return None
    audio_features = torch.stack([x[0] for x in batch])
    prompts  = [x[1] for x in batch]
    captions = [x[2] for x in batch]
    sources  = [x[3] for x in batch]
    label_sets = [x[4] for x in batch]
    tasks = [x[5] for x in batch]                 # âœ¨
    return audio_features, prompts, captions, sources, label_sets, tasks


def save_lora_adapter(model, epoch):
    save_path = os.path.join(LORA_SAVE_DIR, f"epoch_{epoch}/")
    os.makedirs(save_path, exist_ok=True)
    if hasattr(model.llm, "save_pretrained"):
        model.llm.save_pretrained(save_path)
        print(f"âœ… LoRA adapter saved to '{save_path}'")


def save_captions(captions, sources, epoch):
    os.makedirs(CAPTION_SAVE_DIR, exist_ok=True)
    shrimp_data = []
    for cap, src in zip(captions, sources):
        item = {"caption": cap, "source": src}
        shrimp_data.append(item)
    with open(os.path.join(CAPTION_SAVE_DIR, f"shrimp_epoch_{epoch}.jsonl"), "w") as f:
        for item in shrimp_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
import re

def _simple_tokenize(text: str):
    # ä¸ä¾è³´ nltk è³‡æ–™æª”ï¼Œé¿å…ç’°å¢ƒç¼ºå°‘ punktã€‚
    # å°‡å­—æ¯æ•¸å­—åºåˆ—èˆ‡å–®ä¸€ç¬¦è™Ÿåˆ†é–‹ï¼šå¦‚ ["A", "faint", "rustling", ",", "sound", "..."]
    return re.findall(r"\w+|[^\w\s]", text, flags=re.UNICODE)

def run_validation(model, dataloader, tokenizer, config, epoch):
    import re
    model.eval()
    total_meteor = 0.0
    meteor_count = 0

    # çµ±è¨ˆåˆ†é¡çš„ micro-F1ï¼ˆ= æ­£ç¢ºç‡ï¼‰
    cls_correct = 0
    cls_total = 0

    for batch in dataloader:
        if batch is None: 
            continue
        audio_features, prompts, captions, _, label_sets, tasks = batch
        audio_features = audio_features.to(config.device)

        for i in range(len(prompts)):
            task = tasks[i]
            label_set = label_sets[i] or []
            prompt  = prompts[i:i+1]
            caption = captions[i] or ""

            # 1) tokenize
            tokenized = tokenizer(
                prompt, return_tensors="pt",
                padding=True, truncation=True, max_length=config.max_length
            )
            input_ids = tokenized["input_ids"].to(config.device)
            attention_mask = tokenized["attention_mask"].to(config.device)
            if input_ids.size(1) == 0 or attention_mask.size(1) == 0:
                print("âš ï¸ Warning: Empty input_ids/attention_mask; skip.")
                continue

            # 2) æ®˜å·®æ³¨å…¥
            inputs_embeds = model.llm.get_input_embeddings()(input_ids)  # [1, T, H]
            end_token_id  = tokenizer.convert_tokens_to_ids("<end_of_audio>")
            projected, alpha = model.audio_projector(audio_features[i:i+1])  # [1, H], scalar
            pos = (input_ids[0] == end_token_id).nonzero(as_tuple=False)
            if len(pos) > 0:
                j = pos[0].item()
                inputs_embeds[0, j, :] = inputs_embeds[0, j, :] + alpha * projected[0]

            # 3) ç”¢ç”Ÿ / æ‰“åˆ†
            if task == "cls" and len(label_set) > 0:
                # åˆ†é¡ï¼šç”¨ logprob è¨ˆåˆ†é¸æœ€å¤§è€…
                candidate_texts = label_set
                pred, _ = score_labels_by_logprob(
                    model, tokenizer, inputs_embeds, attention_mask, candidate_texts, config.device
                )
            else:
                # QAï¼šè‡ªç”±ç”Ÿæˆ
                with torch.no_grad():
                    out = model.llm.generate(
                        input_ids=input_ids,
                        inputs_embeds=inputs_embeds,
                        attention_mask=attention_mask,
                        max_new_tokens=64,
                        do_sample=False,
                        return_dict_in_generate=True
                    )
                    generated_ids = out.sequences
                new_len = inputs_embeds.shape[1]
                new_tokens = generated_ids[:, new_len:]
                preds = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)
                pred = (preds[0].strip() if len(preds) > 0 else "")

            # æ­£è¦åŒ–æ–‡å­—
            def _clean_text(s: str) -> str:
                s = re.sub(r"<\|.*?\|>", "", s)
                s = s.replace("<end_of_audio>", "")
                s = re.sub(r"\s+", " ", s).strip()
                return s

            q_raw = prompts[i]
            m = re.search(r"<\|user\|>\s*(.*?)\s*<\|assistant\|>", q_raw, flags=re.DOTALL)
            q_show = _clean_text(m.group(1) if m else q_raw)

            ref_show  = _clean_text(caption)
            pred_show = _clean_text(pred)

            # é¡¯ç¤º + è¨ˆåˆ†
            print(f"ğŸ“ QUESTION: {q_show}")
            print(f"ğŸ”¹REF: {ref_show}")
            print(f"ğŸ”¸PRED: {pred_show}")

            if task == "cls":
                # é€ç­† F1ï¼ˆå–®æ¨™ç±¤ â†’ å®Œå…¨ç›¸åŒ=1ï¼Œå¦å‰‡=0ï¼‰
                ref_norm  = ref_show.lower()
                pred_norm = pred_show.lower()
                f1_inst = 1.0 if pred_norm == ref_norm and ref_norm != "" else 0.0
                print(f"ğŸ¯ F1score: {f1_inst:.4f}\n")

                cls_total += 1
                cls_correct += int(f1_inst == 1.0)
            else:
                # QA ç”¨ METEOR
                pred_norm = re.sub(r"\s+", " ", pred_show.lower())
                ref_norm  = re.sub(r"\s+", " ", ref_show.lower())

                pred_toks = _simple_tokenize(pred_norm)
                ref_toks  = _simple_tokenize(ref_norm)

                score = meteor_score([ref_toks], pred_toks)
                print(f"ğŸ¯ METEOR: {score:.4f}\n")

                total_meteor += float(score)
                meteor_count += 1

    avg_meteor = (total_meteor / meteor_count) if meteor_count > 0 else 0.0
    epoch_f1 = (cls_correct / cls_total) if cls_total > 0 else 0.0

    # æœŸæœ«å½™ç¸½åˆ—å°ï¼ˆå…©å€‹æŒ‡æ¨™å„è‡ªåªåœ¨æœ‰è³‡æ–™æ™‚é¡¯ç¤ºï¼‰
    tail = []
    tail.append(f"ğŸ§ª METEOR è©•åˆ† @ epoch {epoch}: {avg_meteor:.4f}")
    tail.append(f"F1score è©•åˆ† @ epoch {epoch}: {epoch_f1:.4f}")
    print("  ".join(tail))

    model.train()
    return avg_meteor  # ä»å›å‚³ METEORï¼ˆè‹¥éœ€è¦ä¹Ÿå¯æ”¹æˆå›å‚³ (avg_meteor, epoch_f1)ï¼‰



def train_instruction_tuning(train_path, val_path, source_to_audio_dir, clap_model_name, llm_model_name,
                             processor, config, tokenizer, batch_size, epochs):
    os.makedirs(PROJECTOR_SAVE_DIR, exist_ok=True)
    global model
    model = MultiModalInstructionTuningModel(clap_model_name, llm_model_name, config, tokenizer).to(config.device)

    # é€™é‚Šè¼‰å…¥finetune éçš„ CLAP æ¬Šé‡
    CLAP_CKPT_PATH = "/mnt/disk6/clap_finetune/shrimp_full/best.ckpt"
    load_clap_weights_into_model(model.clap_model, CLAP_CKPT_PATH)

    # é€™é‚Šå‡çµ CLAP æ¬Šé‡
    for p in model.clap_model.parameters():
        p.requires_grad = False

    # é€™é‚Šè¼‰å…¥è¨“ç·´èˆ‡é©—è­‰è³‡æ–™
    train_dataset = AudioInstructionDataset(train_path, source_to_audio_dir, model, max_length=config.max_length)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = AudioInstructionDataset(val_path, source_to_audio_dir, model, max_length=config.max_length)
    val_loader = DataLoader(val_loader, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        all_captions, all_sources = [], []
        print(f"\nğŸ” Epoch {epoch+1}/{epochs}")
        for batch in train_loader:
            if batch is None: continue
            audio_features, prompts, captions, sources, label_sets, tasks = batch  # âœ¨
            audio_features = audio_features.to(config.device)
            outputs = model(audio_features, prompts, captions)
            loss = outputs.loss
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            all_captions.extend(captions)
            all_sources.extend(sources)
        print(f"ğŸ“‰ Loss: {total_loss / max(1,len(train_loader)):.4f}")
        run_validation(model, val_loader, tokenizer, config, epoch+1)
        if (epoch + 1) % 20 == 0:
            torch.save(model.state_dict(), os.path.join(PROJECTOR_SAVE_DIR, f"projector_epoch_{epoch+1}.pth"))
            save_lora_adapter(model, epoch+1)
        save_captions(all_captions, all_sources, epoch+1)

# ä¸»ç¨‹å¼ç¢¼
if __name__ == "__main__":
    TRAIN_PATH = "/mnt/disk5/shrimp_walk/train.split.50.jsonl"
    VAL_PATH   = "/mnt/disk5/shrimp_walk/val.sys.enq.jsonl"
    SOURCE_TO_AUDIO_DIR = {"audio_path": "/mnt/disk5/shrimp_walk/wav_output/", "user_csv": "/mnt/disk5/shrimp_walk/wav_output/"}
    CLAP_MODEL_NAME = "laion/clap-htsat-unfused"
    LLM_MODEL_NAME  = "microsoft/Phi-3.5-mini-instruct"
    processor = AutoProcessor.from_pretrained(CLAP_MODEL_NAME)
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({"additional_special_tokens": ["<end_of_audio>"]})
    train_instruction_tuning(
        train_path=TRAIN_PATH,
        val_path=VAL_PATH,
        source_to_audio_dir=SOURCE_TO_AUDIO_DIR,
        clap_model_name=CLAP_MODEL_NAME,
        llm_model_name=LLM_MODEL_NAME,
        processor=processor,
        config=config,
        tokenizer=tokenizer,
        batch_size=8,
        epochs=100
    )
